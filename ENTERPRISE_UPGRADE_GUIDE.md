# ZeroSite v4.0 - Enterprise Upgrade Complete Guide
# Phases 1-4: Security, Database, Advanced Features, Deployment

**Author**: ZeroSite Development Team  
**Date**: 2025-12-27  
**Version**: 5.0.0  
**Status**: ğŸŸ¢ Phase 1 COMPLETE | ğŸ“‹ Phases 2-4 BLUEPRINT

---

## âœ… Phase 1: Security & Authentication (COMPLETE)

### Implemented Features

#### 1. JWT Authentication System âœ…
- **Access Tokens**: 30ë¶„ ë§Œë£Œ
- **Refresh Tokens**: 7ì¼ ë§Œë£Œ  
- **Password Hashing**: bcrypt ì•Œê³ ë¦¬ì¦˜
- **Token Validation**: jose ë¼ì´ë¸ŒëŸ¬ë¦¬

**Files Created:**
- `app/core/security.py` - JWT ìƒì„±/ê²€ì¦, ë¹„ë°€ë²ˆí˜¸ í•´ì‹±
- `app/core/auth_deps.py` - FastAPI ì¸ì¦ ì˜ì¡´ì„±
- `api_server_secured.py` - ë³´ì•ˆ ê°•í™” API ì„œë²„

**Test Credentials:**
```
Admin: admin / admin123
Demo:  demo / demo123
```

#### 2. API Key Management âœ…
- API í‚¤ ìƒì„± (zerosite_xxx í˜•ì‹)
- SHA256 í•´ì‹± ì €ì¥
- ë§Œë£Œì¼ ì„¤ì • ê°€ëŠ¥
- ì‚¬ìš© í†µê³„ ì¶”ì  (usage_count, last_used)

**Endpoints:**
```
POST /api/v1/auth/api-keys    - API í‚¤ ìƒì„±
GET  /api/v1/auth/api-keys    - ë‚´ API í‚¤ ëª©ë¡
```

#### 3. Rate Limiting âœ…
- **Slowapi** í†µí•©
- IP ê¸°ë°˜ ì œí•œ
- API í‚¤ ê¸°ë°˜ ì œí•œ (1000 requests/hour)
- ì—”ë“œí¬ì¸íŠ¸ë³„ ì»¤ìŠ¤í…€ ì œí•œ

**Files Created:**
- `app/core/middleware.py` - Rate limiting, ë¡œê¹…, ë³´ì•ˆ í—¤ë”

#### 4. Security Headers âœ…
```
X-Content-Type-Options: nosniff
X-Frame-Options: DENY
X-XSS-Protection: 1; mode=block
Strict-Transport-Security: max-age=31536000
Content-Security-Policy: default-src 'self'
```

#### 5. Authentication Endpoints âœ…
```
POST /api/v1/auth/token       - ë¡œê·¸ì¸ (JWT ë°œê¸‰)
GET  /api/v1/auth/me          - í˜„ì¬ ì‚¬ìš©ì ì •ë³´
POST /api/v1/auth/api-keys    - API í‚¤ ìƒì„±
GET  /api/v1/auth/api-keys    - API í‚¤ ëª©ë¡
```

### Usage Example

#### JWT Authentication
```bash
# 1. ë¡œê·¸ì¸
curl -X POST "http://localhost:8000/api/v1/auth/token" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "username=admin&password=admin123"

# Response:
{
  "access_token": "eyJhbGciOiJIUzI1NiIs...",
  "refresh_token": "eyJhbGciOiJIUzI1NiIs...",
  "token_type": "bearer"
}

# 2. ì¸ì¦ëœ ìš”ì²­
curl -X GET "http://localhost:8000/api/v1/auth/me" \
  -H "Authorization: Bearer eyJhbGciOiJIUzI1NiIs..."
```

#### API Key Usage
```bash
# 1. API í‚¤ ìƒì„± (JWT í•„ìš”)
curl -X POST "http://localhost:8000/api/v1/auth/api-keys" \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"name": "My API Key", "expires_days": 30}'

# Response:
{
  "api_key": "zerosite_abc123def456...",
  "key_id": "key_12345678",
  "name": "My API Key",
  "message": "âš ï¸ ì´ API í‚¤ë¥¼ ì•ˆì „í•œ ê³³ì— ì €ì¥í•˜ì„¸ìš”!"
}

# 2. API í‚¤ë¡œ ìš”ì²­
curl -X POST "http://localhost:8000/api/v1/analyze" \
  -H "Authorization: Bearer zerosite_abc123def456..." \
  -H "Content-Type: application/json" \
  -d '{...}'
```

---

## ğŸ“‹ Phase 2: Database Integration (BLUEPRINT)

### Planned Features

#### 1. PostgreSQL Setup
**Purpose**: ë°ì´í„° ì˜ì†ì„±, ê´€ê³„í˜• ë°ì´í„° ê´€ë¦¬

**Database Schema:**
```sql
-- Users Table
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    hashed_password VARCHAR(255) NOT NULL,
    full_name VARCHAR(100),
    is_admin BOOLEAN DEFAULT FALSE,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- API Keys Table
CREATE TABLE api_keys (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    key_hash VARCHAR(64) NOT NULL,
    name VARCHAR(100) NOT NULL,
    expires_at TIMESTAMP,
    is_active BOOLEAN DEFAULT TRUE,
    usage_count INTEGER DEFAULT 0,
    last_used TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Analysis Jobs Table
CREATE TABLE analysis_jobs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id),
    status VARCHAR(20) NOT NULL,
    progress INTEGER DEFAULT 0,
    land_info JSONB NOT NULL,
    result JSONB,
    error_message TEXT,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    completed_at TIMESTAMP
);

-- Indexes
CREATE INDEX idx_users_username ON users(username);
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_api_keys_user_id ON api_keys(user_id);
CREATE INDEX idx_api_keys_key_hash ON api_keys(key_hash);
CREATE INDEX idx_analysis_jobs_user_id ON analysis_jobs(user_id);
CREATE INDEX idx_analysis_jobs_status ON analysis_jobs(status);
CREATE INDEX idx_analysis_jobs_created_at ON analysis_jobs(created_at DESC);
```

**SQLAlchemy Models** (`app/models/database.py`):
```python
from sqlalchemy import Column, String, Boolean, Integer, TIMESTAMP, ForeignKey, Text
from sqlalchemy.dialects.postgresql import UUID, JSONB
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
import uuid
from datetime import datetime

Base = declarative_base()

class User(Base):
    __tablename__ = "users"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    username = Column(String(50), unique=True, nullable=False)
    email = Column(String(255), unique=True, nullable=False)
    hashed_password = Column(String(255), nullable=False)
    full_name = Column(String(100))
    is_admin = Column(Boolean, default=False)
    is_active = Column(Boolean, default=True)
    created_at = Column(TIMESTAMP, default=datetime.utcnow)
    updated_at = Column(TIMESTAMP, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    api_keys = relationship("APIKey", back_populates="user", cascade="all, delete-orphan")
    analysis_jobs = relationship("AnalysisJob", back_populates="user")

class APIKey(Base):
    __tablename__ = "api_keys"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id = Column(UUID(as_uuid=True), ForeignKey("users.id"), nullable=False)
    key_hash = Column(String(64), nullable=False)
    name = Column(String(100), nullable=False)
    expires_at = Column(TIMESTAMP)
    is_active = Column(Boolean, default=True)
    usage_count = Column(Integer, default=0)
    last_used = Column(TIMESTAMP)
    created_at = Column(TIMESTAMP, default=datetime.utcnow)
    
    # Relationships
    user = relationship("User", back_populates="api_keys")

class AnalysisJob(Base):
    __tablename__ = "analysis_jobs"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id = Column(UUID(as_uuid=True), ForeignKey("users.id"))
    status = Column(String(20), nullable=False)
    progress = Column(Integer, default=0)
    land_info = Column(JSONB, nullable=False)
    result = Column(JSONB)
    error_message = Column(Text)
    created_at = Column(TIMESTAMP, default=datetime.utcnow)
    updated_at = Column(TIMESTAMP, default=datetime.utcnow, onupdate=datetime.utcnow)
    completed_at = Column(TIMESTAMP)
    
    # Relationships
    user = relationship("User", back_populates="analysis_jobs")
```

**Database Connection** (`app/core/database.py`):
```python
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
import os

# Database URL (í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
DATABASE_URL = os.getenv(
    "DATABASE_URL",
    "postgresql+asyncpg://zerosite:password@localhost/zerosite_db"
)

# Async engine
engine = create_async_engine(
    DATABASE_URL,
    echo=True,
    pool_size=20,
    max_overflow=0
)

# Async session maker
async_session_maker = sessionmaker(
    engine,
    class_=AsyncSession,
    expire_on_commit=False
)

# Dependency
async def get_db():
    async with async_session_maker() as session:
        yield session
```

#### 2. Alembic Migrations
**Purpose**: ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ë²„ì „ ê´€ë¦¬

**Setup:**
```bash
# ì´ˆê¸°í™”
alembic init alembic

# ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒì„±
alembic revision --autogenerate -m "Initial schema"

# ì ìš©
alembic upgrade head

# ë¡¤ë°±
alembic downgrade -1
```

#### 3. Redis Caching
**Purpose**: ì„±ëŠ¥ í–¥ìƒ, ì„¸ì…˜ ê´€ë¦¬, Rate Limiting

**Redis Configuration** (`app/core/redis_client.py`):
```python
import redis.asyncio as aioredis
import json
from typing import Optional, Any

class RedisClient:
    def __init__(self, url: str = "redis://localhost:6379"):
        self.redis = aioredis.from_url(url, decode_responses=True)
    
    async def get(self, key: str) -> Optional[str]:
        """Get value by key"""
        return await self.redis.get(key)
    
    async def set(self, key: str, value: Any, ex: int = None):
        """Set key-value with optional expiration"""
        await self.redis.set(key, json.dumps(value), ex=ex)
    
    async def delete(self, key: str):
        """Delete key"""
        await self.redis.delete(key)
    
    async def exists(self, key: str) -> bool:
        """Check if key exists"""
        return await self.redis.exists(key)
    
    async def incr(self, key: str) -> int:
        """Increment counter"""
        return await self.redis.incr(key)
    
    async def expire(self, key: str, seconds: int):
        """Set expiration"""
        await self.redis.expire(key, seconds)

# Global instance
redis_client = RedisClient()
```

**Caching Examples:**
```python
# ë¶„ì„ ê²°ê³¼ ìºì‹± (1ì‹œê°„)
await redis_client.set(
    f"analysis:{job_id}",
    analysis_result,
    ex=3600
)

# Rate limiting
key = f"rate_limit:{api_key}:{datetime.now().hour}"
count = await redis_client.incr(key)
await redis_client.expire(key, 3600)

if count > 1000:
    raise HTTPException(status_code=429, detail="Rate limit exceeded")

# ì„¸ì…˜ ê´€ë¦¬
await redis_client.set(
    f"session:{session_id}",
    user_data,
    ex=1800  # 30ë¶„
)
```

#### 4. Session Management
**Purpose**: ì‚¬ìš©ì ì„¸ì…˜ ì¶”ì , ë¡œê·¸ì¸ ìƒíƒœ ê´€ë¦¬

**Session Store** (`app/core/session.py`):
```python
from fastapi import Request, Response
import secrets
from typing import Dict, Any

class SessionManager:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.session_prefix = "session:"
        self.session_ttl = 1800  # 30ë¶„
    
    async def create_session(self, user_id: str, data: Dict[str, Any]) -> str:
        """Create new session"""
        session_id = secrets.token_urlsafe(32)
        session_key = f"{self.session_prefix}{session_id}"
        
        session_data = {
            "user_id": user_id,
            **data
        }
        
        await self.redis.set(session_key, session_data, ex=self.session_ttl)
        return session_id
    
    async def get_session(self, session_id: str) -> Optional[Dict[str, Any]]:
        """Get session data"""
        session_key = f"{self.session_prefix}{session_id}"
        return await self.redis.get(session_key)
    
    async def delete_session(self, session_id: str):
        """Delete session"""
        session_key = f"{self.session_prefix}{session_id}"
        await self.redis.delete(session_key)
```

---

## ğŸ“‹ Phase 3: Advanced Features (BLUEPRINT)

### 1. WebSocket Real-time Updates
**Purpose**: ì‹¤ì‹œê°„ ì–‘ë°©í–¥ í†µì‹ , ì§„í–‰ ìƒí™© í‘¸ì‹œ

**WebSocket Server** (`app/websocket/connection_manager.py`):
```python
from fastapi import WebSocket
from typing import Dict, List

class ConnectionManager:
    def __init__(self):
        self.active_connections: Dict[str, List[WebSocket]] = {}
    
    async def connect(self, websocket: WebSocket, user_id: str):
        await websocket.accept()
        if user_id not in self.active_connections:
            self.active_connections[user_id] = []
        self.active_connections[user_id].append(websocket)
    
    def disconnect(self, websocket: WebSocket, user_id: str):
        self.active_connections[user_id].remove(websocket)
    
    async def send_personal_message(self, message: dict, user_id: str):
        if user_id in self.active_connections:
            for connection in self.active_connections[user_id]:
                await connection.send_json(message)
    
    async def broadcast(self, message: dict):
        for connections in self.active_connections.values():
            for connection in connections:
                await connection.send_json(message)

manager = ConnectionManager()

@app.websocket("/ws/{user_id}")
async def websocket_endpoint(websocket: WebSocket, user_id: str):
    await manager.connect(websocket, user_id)
    try:
        while True:
            data = await websocket.receive_text()
            await manager.send_personal_message(
                {"message": f"You sent: {data}"},
                user_id
            )
    except WebSocketDisconnect:
        manager.disconnect(websocket, user_id)
```

**Client-side JavaScript:**
```javascript
// WebSocket ì—°ê²°
const ws = new WebSocket(`ws://localhost:8000/ws/${userId}`);

ws.onopen = () => {
    console.log('WebSocket connected');
};

ws.onmessage = (event) => {
    const data = JSON.parse(event.data);
    
    if (data.type === 'analysis_progress') {
        updateProgressBar(data.progress);
    } else if (data.type === 'analysis_complete') {
        showResults(data.result);
    }
};

ws.onerror = (error) => {
    console.error('WebSocket error:', error);
};

ws.onclose = () => {
    console.log('WebSocket disconnected');
};
```

### 2. File Upload (Excel Batch Analysis)
**Purpose**: ì—‘ì…€ íŒŒì¼ë¡œ ë‹¤ì¤‘ ë¶€ì§€ ì¼ê´„ ë¶„ì„

**Upload Endpoint:**
```python
@app.post("/api/v1/upload/excel")
async def upload_excel_file(
    file: UploadFile = File(...),
    current_user: User = Depends(get_current_active_user)
):
    """
    ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ ë° ì¼ê´„ ë¶„ì„
    
    Expected Excel columns:
    - ì§€ë²ˆ, ì£¼ì†Œ, ë©´ì (ã¡), ìš©ë„ì§€ì—­, ìš©ì ë¥ , ê±´íìœ¨, ì ‘ë„í­
    """
    # íŒŒì¼ ê²€ì¦
    if not file.filename.endswith(('.xlsx', '.xls')):
        raise HTTPException(400, "Excel íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    
    # íŒŒì¼ ì½ê¸°
    contents = await file.read()
    
    # openpyxlë¡œ íŒŒì‹±
    import openpyxl
    from io import BytesIO
    
    wb = openpyxl.load_workbook(BytesIO(contents))
    ws = wb.active
    
    sites = []
    for row in ws.iter_rows(min_row=2, values_only=True):
        site_data = {
            "parcel_id": row[0],
            "address": row[1],
            "area_sqm": float(row[2]),
            "zone_type": row[3],
            "far": float(row[4]),
            "bcr": float(row[5]),
            "road_width": float(row[6])
        }
        sites.append(site_data)
    
    # ì¼ê´„ ë¶„ì„ ì‘ì—… ìƒì„±
    batch_id = str(uuid.uuid4())
    for site in sites:
        # ê° ë¶€ì§€ì— ëŒ€í•´ ë¶„ì„ ì‘ì—… ìƒì„±
        pass
    
    return {
        "batch_id": batch_id,
        "total_sites": len(sites),
        "message": "ì¼ê´„ ë¶„ì„ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
    }
```

### 3. Email Notifications
**Purpose**: ë¶„ì„ ì™„ë£Œ ì•Œë¦¼, ë¦¬í¬íŠ¸ ë°œì†¡

**Email Configuration** (`app/core/email.py`):
```python
from fastapi_mail import FastMail, MessageSchema, ConnectionConfig
import os

conf = ConnectionConfig(
    MAIL_USERNAME=os.getenv("MAIL_USERNAME"),
    MAIL_PASSWORD=os.getenv("MAIL_PASSWORD"),
    MAIL_FROM=os.getenv("MAIL_FROM"),
    MAIL_PORT=587,
    MAIL_SERVER="smtp.gmail.com",
    MAIL_STARTTLS=True,
    MAIL_SSL_TLS=False,
    USE_CREDENTIALS=True
)

fast_mail = FastMail(conf)

async def send_analysis_complete_email(
    email: str,
    job_id: str,
    result_url: str
):
    """ë¶„ì„ ì™„ë£Œ ì´ë©”ì¼ ë°œì†¡"""
    html = f"""
    <html>
    <body>
        <h2>ZeroSite ë¶„ì„ ì™„ë£Œ</h2>
        <p>ìš”ì²­í•˜ì‹  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.</p>
        <p>Job ID: <code>{job_id}</code></p>
        <p><a href="{result_url}">ê²°ê³¼ ë³´ê¸°</a></p>
    </body>
    </html>
    """
    
    message = MessageSchema(
        subject="ZeroSite ë¶„ì„ ì™„ë£Œ",
        recipients=[email],
        body=html,
        subtype="html"
    )
    
    await fast_mail.send_message(message)
```

### 4. Task Scheduling (APScheduler)
**Purpose**: ì£¼ê¸°ì  ì‘ì—…, ì˜ˆì•½ ë¶„ì„

**Scheduler Setup** (`app/core/scheduler.py`):
```python
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger

scheduler = AsyncIOScheduler()

async def cleanup_old_jobs():
    """24ì‹œê°„ ì´ìƒ ëœ ì™„ë£Œ/ì‹¤íŒ¨ ì‘ì—… ì‚­ì œ"""
    cutoff = datetime.now() - timedelta(days=1)
    # DBì—ì„œ ì‚­ì œ
    pass

async def send_daily_report():
    """ì¼ì¼ ë¦¬í¬íŠ¸ ë°œì†¡"""
    # í†µê³„ ì§‘ê³„ ë° ì´ë©”ì¼ ë°œì†¡
    pass

# ìŠ¤ì¼€ì¤„ ë“±ë¡
scheduler.add_job(
    cleanup_old_jobs,
    CronTrigger(hour=2, minute=0),  # ë§¤ì¼ ìƒˆë²½ 2ì‹œ
    id="cleanup_old_jobs"
)

scheduler.add_job(
    send_daily_report,
    CronTrigger(hour=9, minute=0),  # ë§¤ì¼ ì˜¤ì „ 9ì‹œ
    id="send_daily_report"
)

# ì‹œì‘
scheduler.start()
```

---

## ğŸ“‹ Phase 4: Deployment & DevOps (BLUEPRINT)

### 1. Docker Containerization

**Dockerfile:**
```dockerfile
FROM python:3.9-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Expose port
EXPOSE 8000

# Run application
CMD ["uvicorn", "api_server_secured:app", "--host", "0.0.0.0", "--port", "8000"]
```

**docker-compose.yml:**
```yaml
version: '3.8'

services:
  web:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql+asyncpg://zerosite:password@db/zerosite_db
      - REDIS_URL=redis://redis:6379
    depends_on:
      - db
      - redis
    volumes:
      - ./output:/app/output

  db:
    image: postgres:14
    environment:
      POSTGRES_USER: zerosite
      POSTGRES_PASSWORD: password
      POSTGRES_DB: zerosite_db
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - web

volumes:
  postgres_data:
  redis_data:
```

### 2. Kubernetes Deployment

**deployment.yaml:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zerosite-api
  labels:
    app: zerosite
spec:
  replicas: 3
  selector:
    matchLabels:
      app: zerosite
  template:
    metadata:
      labels:
        app: zerosite
    spec:
      containers:
      - name: api
        image: zerosite/api:latest
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: zerosite-secrets
              key: database-url
        - name: REDIS_URL
          valueFrom:
            configMapKeyRef:
              name: zerosite-config
              key: redis-url
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: zerosite-service
spec:
  selector:
    app: zerosite
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
```

### 3. CI/CD Pipeline (GitHub Actions)

**.github/workflows/deploy.yml:**
```yaml
name: Deploy ZeroSite

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-cov
    
    - name: Run tests
      run: |
        pytest tests/ --cov=app --cov-report=xml
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3

  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Build Docker image
      run: |
        docker build -t zerosite/api:${{ github.sha }} .
        docker tag zerosite/api:${{ github.sha }} zerosite/api:latest
    
    - name: Push to registry
      run: |
        echo "${{ secrets.DOCKER_PASSWORD }}" | docker login -u "${{ secrets.DOCKER_USERNAME }}" --password-stdin
        docker push zerosite/api:${{ github.sha }}
        docker push zerosite/api:latest

  deploy:
    needs: build
    runs-on: ubuntu-latest
    steps:
    - name: Deploy to Kubernetes
      uses: azure/k8s-deploy@v4
      with:
        manifests: |
          k8s/deployment.yaml
          k8s/service.yaml
        images: |
          zerosite/api:${{ github.sha }}
```

### 4. Monitoring & Logging

**Prometheus Configuration** (`prometheus.yml`):
```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'zerosite-api'
    static_configs:
      - targets: ['localhost:8000']
    metrics_path: /metrics
```

**Grafana Dashboard:**
- Request Rate
- Response Time (p50, p95, p99)
- Error Rate
- Active Connections
- Database Query Performance
- Redis Cache Hit Rate

**ELK Stack (Elasticsearch, Logstash, Kibana):**
```yaml
# logstash.conf
input {
  file {
    path => "/var/log/zerosite/*.log"
    start_position => "beginning"
  }
}

filter {
  json {
    source => "message"
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "zerosite-logs-%{+YYYY.MM.dd}"
  }
}
```

---

## ğŸ“Š Summary

### Completed (Phase 1)
- âœ… JWT Authentication
- âœ… API Key Management
- âœ… Rate Limiting
- âœ… Security Headers
- âœ… Request Logging
- âœ… Error Handling

### Planned (Phases 2-4)
- ğŸ“‹ PostgreSQL Integration
- ğŸ“‹ Redis Caching
- ğŸ“‹ Session Management
- ğŸ“‹ WebSocket Real-time
- ğŸ“‹ File Upload
- ğŸ“‹ Email Notifications
- ğŸ“‹ Task Scheduling
- ğŸ“‹ Docker Containers
- ğŸ“‹ Kubernetes Deployment
- ğŸ“‹ CI/CD Pipeline
- ğŸ“‹ Monitoring & Logging

### Deployment Checklist

#### Pre-Production
- [ ] Environment variables configured
- [ ] Database migrations applied
- [ ] Redis connection tested
- [ ] SSL certificates installed
- [ ] CORS origins updated
- [ ] Rate limits tuned
- [ ] Monitoring dashboards set up

#### Production
- [ ] Load balancer configured
- [ ] Auto-scaling enabled
- [ ] Backup strategy implemented
- [ ] Disaster recovery plan
- [ ] Security audit completed
- [ ] Performance testing done
- [ ] Documentation updated

---

## ğŸš€ Next Steps

1. **Implement Phase 2**: Database integration (PostgreSQL + Redis)
2. **Implement Phase 3**: Advanced features (WebSocket, File Upload, Email)
3. **Implement Phase 4**: Deployment (Docker, Kubernetes, CI/CD)
4. **Testing**: Comprehensive testing at each phase
5. **Deployment**: Staged rollout to production

---

*End of Enterprise Upgrade Guide*
