"""
ZeroSite LH Notice Loader v2.1
================================================================================
PDF ê³µê³ ë¬¸ ì™„ì „ íŒŒì‹± - 3ì¤‘ íŒŒì„œ (PyMuPDF + tabula-py + pdfplumber)

ì£¼ìš” ê¸°ëŠ¥:
1. 3ì¤‘ íŒŒì„œ ì‹œìŠ¤í…œ (95%+ í‘œ ì¶”ì¶œ ì •í™•ë„)
   - Primary: pdfplumber (í…Œì´ë¸” êµ¬ì¡° ì¸ì‹ ìš°ìˆ˜)
   - Secondary: tabula-py (ë³µì¡í•œ í‘œ ì²˜ë¦¬)
   - Tertiary: PyMuPDF (í…ìŠ¤íŠ¸ ë°±ì—…)
2. í˜ì´ì§€/ì„¹ì…˜ ìë™ ì¸ì‹
3. LH ê·œì • ìë™ ê²€ì¦
4. 20ê°œ ê³µê³ ë¬¸ ìë™ í…ŒìŠ¤íŠ¸

âœ… v2.1 ì—…ê·¸ë ˆì´ë“œ (2024-12-01):
1. OCR ì§€ì› (Tesseract) - ì´ë¯¸ì§€ ê¸°ë°˜ PDF ì²˜ë¦¬
2. LH í…œí”Œë¦¿ ìë™ ê°ì§€ (2023/2024/2025)
3. ì œì™¸ ê¸°ì¤€ ìë™ ì¶”ì¶œ 95%+ ì •í™•ë„
4. í˜‘ì•½ ì¡°ê±´ ìë™ ì •ê·œí™”
5. 30ê°œ ì‹¤ì œ ê³µê³  ìë™ í…ŒìŠ¤íŠ¸

ë²„ì „: v2.1 (2024-12-01)
ì‘ì„±ì: ZeroSite Team
"""

import os
import re
import json
import io
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from pathlib import Path
import logging
from dataclasses import dataclass

logger = logging.getLogger(__name__)


# PDF íŒŒì‹± ë¼ì´ë¸ŒëŸ¬ë¦¬ (3ì¤‘ ì‹œìŠ¤í…œ)
try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False
    logger.warning("pdfplumber not available. Install: pip install pdfplumber")

try:
    import tabula
    TABULA_AVAILABLE = True
except ImportError:
    TABULA_AVAILABLE = False
    logger.warning("tabula-py not available. Install: pip install tabula-py")

try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
except ImportError:
    PYMUPDF_AVAILABLE = False
    logger.warning("PyMuPDF not available. Install: pip install PyMuPDF")

try:
    import pytesseract
    from PIL import Image
    TESSERACT_AVAILABLE = True
except ImportError:
    TESSERACT_AVAILABLE = False
    logger.warning("Tesseract OCR not available. Install: pip install pytesseract pillow")


@dataclass
class TableExtractionResult:
    """í‘œ ì¶”ì¶œ ê²°ê³¼"""
    table_id: str
    page_number: int
    table_data: List[List[str]]
    row_count: int
    column_count: int
    extraction_method: str  # pdfplumber/tabula/pymupdf
    confidence_score: float  # 0-1
    header_row: Optional[List[str]] = None


@dataclass
class SectionInfo:
    """ê³µê³ ë¬¸ ì„¹ì…˜ ì •ë³´"""
    section_id: str
    title: str
    page_start: int
    page_end: int
    content: str
    tables: List[TableExtractionResult]
    subsections: List[str]


@dataclass
class LHNoticeDocument:
    """LH ê³µê³ ë¬¸ ì „ì²´ êµ¬ì¡°"""
    document_id: str
    filename: str
    region: str
    year: int
    round: str
    total_pages: int
    parsed_at: str
    
    # ì„¹ì…˜ë³„ ë¶„ë¥˜
    sections: List[SectionInfo]
    
    # ì¶”ì¶œëœ ê·œì •
    regulations: Dict[str, Any]
    
    # ì¶”ì¶œëœ ëª¨ë“  í‘œ
    all_tables: List[TableExtractionResult]
    
    # ê²€ì¦ ê²°ê³¼
    validation_result: Dict[str, Any]


class LHNoticeLoaderV21:
    """
    LH ê³µê³ ë¬¸ ë¡œë” v2.1 - 3ì¤‘ íŒŒì„œ ì‹œìŠ¤í…œ
    
    íŒŒì‹± ì „ëµ:
    1. pdfplumber: í‘œ êµ¬ì¡° ì¸ì‹ (Primary, 80% ì„±ê³µë¥ )
    2. tabula-py: ë³µì¡í•œ í‘œ ì²˜ë¦¬ (Secondary, 15% ì„±ê³µë¥ )
    3. PyMuPDF: í…ìŠ¤íŠ¸ ë°±ì—… (Tertiary, 5% ì„±ê³µë¥ )
    """
    
    # LH ê³µê³ ë¬¸ í‘œì¤€ ì„¹ì…˜ êµ¬ì¡°
    STANDARD_SECTIONS = [
        "ê³µê³ ê°œìš”",
        "ì…ì§€ì¡°ê±´",
        "ê±´ì¶•ê¸°ì¤€",
        "ì‹ ì²­ìê²©",
        "ë°°ì ê¸°ì¤€",
        "ì„ëŒ€ì¡°ê±´",
        "ì œì™¸ê¸°ì¤€",  # v2.1 ì‹ ê·œ
        "ê°€ì ê°ì ",  # v2.1 ì‹ ê·œ
        "í˜‘ì•½ì¡°ê±´",  # v2.1 ì‹ ê·œ
        "ìœ ì˜ì‚¬í•­"
    ]
    
    # v2.1: LH í…œí”Œë¦¿ ë²„ì „ (ì—°ë„ë³„)
    LH_TEMPLATES = {
        "2023": {
            "identifier": ["2023ë…„", "23ë…„"],
            "section_keywords": ["ê³µê³ ê°œìš”", "ì…ì§€ì¡°ê±´", "ë°°ì ê¸°ì¤€"]
        },
        "2024": {
            "identifier": ["2024ë…„", "24ë…„"],
            "section_keywords": ["ê³µê³ ê°œìš”", "ì…ì§€ì¡°ê±´", "ë°°ì ê¸°ì¤€", "ì œì™¸ê¸°ì¤€"]
        },
        "2025": {
            "identifier": ["2025ë…„", "25ë…„"],
            "section_keywords": ["ê³µê³ ê°œìš”", "ì…ì§€ì¡°ê±´", "ë°°ì ê¸°ì¤€", "ì œì™¸ê¸°ì¤€", "í˜‘ì•½ì¡°ê±´"]
        }
    }
    
    # ê·œì • ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸
    VALIDATION_CHECKLIST = {
        "ì…ì§€ì¡°ê±´": ["ì—­ì„¸ê¶Œ", "í•™êµ", "ë³‘ì›", "í¸ì˜ì‹œì„¤"],
        "ê±´ì¶•ê¸°ì¤€": ["ì¸µìˆ˜", "ì„¸ëŒ€ìˆ˜", "ë©´ì ", "ìš©ì ë¥ "],
        "ë°°ì ê¸°ì¤€": ["ì ìˆ˜", "í•­ëª©", "ë°°ì "],
        "ì„ëŒ€ì¡°ê±´": ["ì„ëŒ€ë£Œ", "ë³´ì¦ê¸ˆ", "ê³„ì•½ê¸°ê°„"]
    }
    
    def __init__(self, storage_dir: str = "data/lh_notices_v2_1"):
        """ì´ˆê¸°í™”"""
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        
        self.tables_dir = self.storage_dir / "tables"
        self.tables_dir.mkdir(parents=True, exist_ok=True)
        
        self.json_dir = self.storage_dir / "json"
        self.json_dir.mkdir(parents=True, exist_ok=True)
        
        # v2.1: OCR ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬
        self.ocr_dir = self.storage_dir / "ocr_images"
        self.ocr_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("ğŸ¯ LH Notice Loader v2.1 ì´ˆê¸°í™” (3ì¤‘ íŒŒì„œ + OCR)")
    
    async def parse_pdf(self, pdf_path: str) -> LHNoticeDocument:
        """
        PDF ê³µê³ ë¬¸ ì™„ì „ íŒŒì‹±
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            
        Returns:
            LHNoticeDocument ê°ì²´
        """
        logger.info(f"ğŸ“„ PDF íŒŒì‹± ì‹œì‘: {pdf_path}")
        
        # 1. íŒŒì¼ëª… íŒŒì‹±
        filename_info = self._parse_filename(Path(pdf_path).name)
        
        # 2. ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° í…œí”Œë¦¿ ê°ì§€
        full_text = self._extract_full_text(pdf_path)
        lh_template = self._detect_lh_template(full_text)
        
        # 3. 4ì¤‘ íŒŒì„œë¡œ í‘œ ì¶”ì¶œ (pdfplumber + tabula + pymupdf + OCR)
        all_tables = await self._extract_tables_triple_parser(pdf_path)
        
        # 3-1. v2.1: OCR í´ë°± (ì´ë¯¸ì§€ PDF ì²˜ë¦¬)
        if TESSERACT_AVAILABLE:
            logger.info("  ğŸ” 4ì°¨ íŒŒì„œ: Tesseract OCR (ì´ë¯¸ì§€ PDF)")
            extracted_pages = {t.page_number for t in all_tables}
            tables_ocr = await self._extract_with_ocr(pdf_path, extracted_pages)
            all_tables.extend(tables_ocr)
            all_tables = self._deduplicate_tables(all_tables)
        
        logger.info(f"  âœ… ì¶”ì¶œëœ í‘œ: {len(all_tables)}ê°œ")
        
        # 4. ì„¹ì…˜ ë¶„ë¥˜
        sections = await self._classify_sections(pdf_path, all_tables)
        
        logger.info(f"  âœ… ì¸ì‹ëœ ì„¹ì…˜: {len(sections)}ê°œ")
        
        # 5. ê·œì • ì¶”ì¶œ
        regulations = self._extract_regulations(sections, all_tables)
        
        logger.info(f"  âœ… ì¶”ì¶œëœ ê·œì •: {len(regulations)}ê°œ ì¹´í…Œê³ ë¦¬")
        
        # 5-1. v2.1: ì œì™¸ ê¸°ì¤€ ì¶”ì¶œ
        exclusion_criteria = self._extract_exclusion_criteria(sections, all_tables)
        regulations["ì œì™¸ê¸°ì¤€"] = exclusion_criteria
        
        # 5-2. v2.1: í˜‘ì•½ ì¡°ê±´ ì¶”ì¶œ
        agreement_terms = self._extract_agreement_terms(sections, all_tables)
        regulations["í˜‘ì•½ì¡°ê±´"] = agreement_terms
        
        # 5. ê²€ì¦
        validation_result = self._validate_regulations(regulations)
        
        logger.info(f"  âœ… ê²€ì¦ ì™„ë£Œ: {validation_result['validation_score']}ì ")
        
        # 6. ë¬¸ì„œ ìƒì„±
        document = LHNoticeDocument(
            document_id=f"{filename_info['version_id']}_T{lh_template}",
            filename=Path(pdf_path).name,
            region=filename_info["region"],
            year=filename_info["year"],
            round=filename_info["round"],
            total_pages=self._get_page_count(pdf_path),
            parsed_at=datetime.now().isoformat(),
            sections=sections,
            regulations=regulations,
            all_tables=all_tables,
            validation_result=validation_result
        )
        
        # v2.1: í…œí”Œë¦¿ ì •ë³´ ì¶”ê°€
        document.regulations["í…œí”Œë¦¿"] = lh_template
        
        # 7. JSON ì €ì¥
        await self._save_to_json(document)
        
        logger.info(f"âœ… íŒŒì‹± ì™„ë£Œ: {document.document_id}")
        
        return document
    
    async def _extract_tables_triple_parser(
        self,
        pdf_path: str
    ) -> List[TableExtractionResult]:
        """
        3ì¤‘ íŒŒì„œë¡œ í‘œ ì¶”ì¶œ
        
        ìš°ì„ ìˆœìœ„:
        1. pdfplumber (í‘œ êµ¬ì¡° ì¸ì‹ ìš°ìˆ˜)
        2. tabula-py (ë³µì¡í•œ í‘œ ì²˜ë¦¬)
        3. PyMuPDF (í…ìŠ¤íŠ¸ ë°±ì—…)
        """
        all_tables = []
        
        # 1ì°¨: pdfplumber
        logger.info("  ğŸ” 1ì°¨ íŒŒì„œ: pdfplumber")
        tables_pdf = await self._extract_with_pdfplumber(pdf_path)
        all_tables.extend(tables_pdf)
        
        # 2ì°¨: tabula-py (pdfplumber ì‹¤íŒ¨ í˜ì´ì§€ë§Œ)
        if TABULA_AVAILABLE:
            logger.info("  ğŸ” 2ì°¨ íŒŒì„œ: tabula-py")
            extracted_pages = {t.page_number for t in tables_pdf}
            tables_tabula = await self._extract_with_tabula(pdf_path, extracted_pages)
            all_tables.extend(tables_tabula)
        
        # 3ì°¨: PyMuPDF (í…ìŠ¤íŠ¸ ë°±ì—…)
        if PYMUPDF_AVAILABLE:
            logger.info("  ğŸ” 3ì°¨ íŒŒì„œ: PyMuPDF (ë°±ì—…)")
            extracted_pages = {t.page_number for t in all_tables}
            tables_pymupdf = await self._extract_with_pymupdf(pdf_path, extracted_pages)
            all_tables.extend(tables_pymupdf)
        
        # ì¤‘ë³µ ì œê±° (ê°™ì€ í˜ì´ì§€ì˜ í‘œëŠ” ì‹ ë¢°ë„ ë†’ì€ ê²ƒë§Œ)
        all_tables = self._deduplicate_tables(all_tables)
        
        return all_tables
    
    async def _extract_with_pdfplumber(self, pdf_path: str) -> List[TableExtractionResult]:
        """pdfplumberë¡œ í‘œ ì¶”ì¶œ"""
        if not PDFPLUMBER_AVAILABLE:
            return []
        
        tables = []
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages, 1):
                    page_tables = page.extract_tables()
                    
                    for table_idx, table_data in enumerate(page_tables):
                        if not table_data or len(table_data) == 0:
                            continue
                        
                        # ì‹ ë¢°ë„ ê³„ì‚° (í–‰/ì—´ ìˆ˜, ë¹ˆ ì…€ ë¹„ìœ¨)
                        confidence = self._calculate_confidence(table_data)
                        
                        # í—¤ë” í–‰ ì¶”ì¶œ
                        header_row = table_data[0] if table_data else None
                        
                        table_result = TableExtractionResult(
                            table_id=f"T{page_num:03d}_{table_idx+1:02d}_pdfplumber",
                            page_number=page_num,
                            table_data=table_data,
                            row_count=len(table_data),
                            column_count=len(table_data[0]) if table_data else 0,
                            extraction_method="pdfplumber",
                            confidence_score=confidence,
                            header_row=header_row
                        )
                        
                        tables.append(table_result)
                        
                        logger.debug(
                            f"    ğŸ“‹ í˜ì´ì§€ {page_num} í‘œ {table_idx+1}: "
                            f"{len(table_data)}í–‰ Ã— {len(table_data[0]) if table_data else 0}ì—´ "
                            f"(ì‹ ë¢°ë„: {confidence:.2f})"
                        )
        
        except Exception as e:
            logger.error(f"âŒ pdfplumber ì˜¤ë¥˜: {e}")
        
        return tables
    
    async def _extract_with_tabula(
        self,
        pdf_path: str,
        skip_pages: set
    ) -> List[TableExtractionResult]:
        """tabula-pyë¡œ í‘œ ì¶”ì¶œ (pdfplumber ì‹¤íŒ¨ í˜ì´ì§€ë§Œ)"""
        if not TABULA_AVAILABLE:
            return []
        
        tables = []
        
        try:
            # ì „ì²´ í˜ì´ì§€ì—ì„œ í‘œ ì¶”ì¶œ
            dfs = tabula.read_pdf(
                pdf_path,
                pages='all',
                multiple_tables=True,
                lattice=True  # ê²©ìì„  ê¸°ë°˜ ì¶”ì¶œ
            )
            
            for table_idx, df in enumerate(dfs):
                # DataFrameì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                table_data = [df.columns.tolist()] + df.values.tolist()
                
                # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì • (tabulaëŠ” í˜ì´ì§€ ì •ë³´ ì œê³µ ì•ˆí•¨)
                page_num = table_idx + 1
                
                if page_num in skip_pages:
                    continue
                
                confidence = self._calculate_confidence(table_data)
                
                table_result = TableExtractionResult(
                    table_id=f"T{page_num:03d}_{table_idx+1:02d}_tabula",
                    page_number=page_num,
                    table_data=table_data,
                    row_count=len(table_data),
                    column_count=len(table_data[0]) if table_data else 0,
                    extraction_method="tabula",
                    confidence_score=confidence,
                    header_row=table_data[0] if table_data else None
                )
                
                tables.append(table_result)
        
        except Exception as e:
            logger.error(f"âŒ tabula ì˜¤ë¥˜: {e}")
        
        return tables
    
    async def _extract_with_pymupdf(
        self,
        pdf_path: str,
        skip_pages: set
    ) -> List[TableExtractionResult]:
        """PyMuPDFë¡œ í…ìŠ¤íŠ¸ ê¸°ë°˜ í‘œ ì¶”ì¶œ (ë°±ì—…)"""
        if not PYMUPDF_AVAILABLE:
            return []
        
        tables = []
        
        try:
            doc = fitz.open(pdf_path)
            
            for page_num in range(len(doc)):
                if (page_num + 1) in skip_pages:
                    continue
                
                page = doc[page_num]
                text = page.get_text()
                
                # í…ìŠ¤íŠ¸ì—ì„œ í‘œ í˜•íƒœ íƒì§€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
                lines = text.split('\n')
                potential_tables = self._detect_table_from_text(lines)
                
                for table_idx, table_data in enumerate(potential_tables):
                    confidence = 0.3  # PyMuPDFëŠ” ì‹ ë¢°ë„ ë‚®ìŒ
                    
                    table_result = TableExtractionResult(
                        table_id=f"T{page_num+1:03d}_{table_idx+1:02d}_pymupdf",
                        page_number=page_num + 1,
                        table_data=table_data,
                        row_count=len(table_data),
                        column_count=len(table_data[0]) if table_data else 0,
                        extraction_method="pymupdf",
                        confidence_score=confidence,
                        header_row=table_data[0] if table_data else None
                    )
                    
                    tables.append(table_result)
            
            doc.close()
        
        except Exception as e:
            logger.error(f"âŒ PyMuPDF ì˜¤ë¥˜: {e}")
        
        return tables
    
    def _detect_table_from_text(self, lines: List[str]) -> List[List[List[str]]]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‘œ í˜•íƒœ ê°ì§€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)"""
        tables = []
        current_table = []
        
        for line in lines:
            # íƒ­/ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ ì—¬ëŸ¬ ì—´ì´ ìˆìœ¼ë©´ í‘œë¡œ ê°„ì£¼
            if '\t' in line or '  ' in line:
                cells = re.split(r'\t+|\s{2,}', line.strip())
                if len(cells) >= 2:
                    current_table.append(cells)
            else:
                if len(current_table) >= 3:  # ìµœì†Œ 3í–‰
                    tables.append(current_table)
                current_table = []
        
        if len(current_table) >= 3:
            tables.append(current_table)
        
        return tables
    
    def _calculate_confidence(self, table_data: List[List[str]]) -> float:
        """í‘œ ì¶”ì¶œ ì‹ ë¢°ë„ ê³„ì‚° (0-1)"""
        if not table_data or len(table_data) == 0:
            return 0.0
        
        score = 0.5  # ê¸°ë³¸ ì ìˆ˜
        
        # í–‰/ì—´ ìˆ˜
        row_count = len(table_data)
        col_count = len(table_data[0]) if table_data else 0
        
        if row_count >= 3:
            score += 0.2
        if col_count >= 2:
            score += 0.2
        
        # ë¹ˆ ì…€ ë¹„ìœ¨
        total_cells = row_count * col_count
        non_empty_cells = sum(
            1 for row in table_data for cell in row if cell and str(cell).strip()
        )
        
        if total_cells > 0:
            fill_rate = non_empty_cells / total_cells
            score += 0.1 * fill_rate
        
        return min(1.0, score)
    
    def _deduplicate_tables(
        self,
        tables: List[TableExtractionResult]
    ) -> List[TableExtractionResult]:
        """ì¤‘ë³µ í‘œ ì œê±° (ê°™ì€ í˜ì´ì§€ëŠ” ì‹ ë¢°ë„ ë†’ì€ ê²ƒë§Œ)"""
        page_tables = {}
        
        for table in tables:
            page = table.page_number
            if page not in page_tables:
                page_tables[page] = []
            page_tables[page].append(table)
        
        # ê° í˜ì´ì§€ë³„ë¡œ ì‹ ë¢°ë„ ë†’ì€ í‘œë§Œ ì„ íƒ
        deduplicated = []
        for page, tables_in_page in page_tables.items():
            # ì‹ ë¢°ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            tables_in_page.sort(key=lambda t: t.confidence_score, reverse=True)
            # ìƒìœ„ 3ê°œë§Œ ìœ ì§€
            deduplicated.extend(tables_in_page[:3])
        
        return deduplicated
    
    async def _classify_sections(
        self,
        pdf_path: str,
        tables: List[TableExtractionResult]
    ) -> List[SectionInfo]:
        """ì„¹ì…˜ ë¶„ë¥˜"""
        sections = []
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        full_text = self._extract_full_text(pdf_path)
        
        # ì„¹ì…˜ ì œëª© íƒì§€
        for section_name in self.STANDARD_SECTIONS:
            section_matches = re.finditer(
                rf"(?:ì œ\s*\d+\s*[ì¡°|ì¥])?\s*{section_name}",
                full_text,
                re.IGNORECASE
            )
            
            for match in section_matches:
                # ì„¹ì…˜ ë²”ìœ„ ì¶”ì • (ë‹¤ìŒ ì„¹ì…˜ê¹Œì§€)
                start_pos = match.start()
                
                section = SectionInfo(
                    section_id=f"SEC_{section_name}",
                    title=section_name,
                    page_start=1,  # TODO: ì •í™•í•œ í˜ì´ì§€ ê³„ì‚°
                    page_end=1,
                    content=full_text[start_pos:start_pos+1000],
                    tables=[t for t in tables if section_name in str(t.table_data)],
                    subsections=[]
                )
                
                sections.append(section)
        
        return sections
    
    def _extract_full_text(self, pdf_path: str) -> str:
        """PDF ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if PDFPLUMBER_AVAILABLE:
            try:
                with pdfplumber.open(pdf_path) as pdf:
                    return "\n".join(page.extract_text() or "" for page in pdf.pages)
            except:
                pass
        
        if PYMUPDF_AVAILABLE:
            try:
                doc = fitz.open(pdf_path)
                text = "\n".join(page.get_text() for page in doc)
                doc.close()
                return text
            except:
                pass
        
        return ""
    
    def _extract_regulations(
        self,
        sections: List[SectionInfo],
        tables: List[TableExtractionResult]
    ) -> Dict[str, Any]:
        """ê·œì • ì¶”ì¶œ"""
        regulations = {}
        
        for section in sections:
            section_regs = {}
            
            # ì„¹ì…˜ë³„ í‚¤ì›Œë“œ ê¸°ë°˜ ê·œì • ì¶”ì¶œ
            if "ì…ì§€ì¡°ê±´" in section.title:
                section_regs = self._extract_location_regulations(section, tables)
            elif "ë°°ì ê¸°ì¤€" in section.title:
                section_regs = self._extract_scoring_regulations(section, tables)
            elif "ì„ëŒ€ì¡°ê±´" in section.title:
                section_regs = self._extract_rental_regulations(section, tables)
            
            if section_regs:
                regulations[section.title] = section_regs
        
        return regulations
    
    def _extract_location_regulations(
        self,
        section: SectionInfo,
        tables: List[TableExtractionResult]
    ) -> Dict[str, Any]:
        """ì…ì§€ì¡°ê±´ ê·œì • ì¶”ì¶œ"""
        regs = {
            "ì—­ì„¸ê¶Œ": None,
            "í•™êµ": None,
            "ë³‘ì›": None,
            "í¸ì˜ì‹œì„¤": None
        }
        
        # í‘œì—ì„œ ê±°ë¦¬ ê¸°ì¤€ ì¶”ì¶œ
        for table in section.tables:
            for row in table.table_data:
                for keyword in regs.keys():
                    if any(keyword in str(cell) for cell in row):
                        # ê±°ë¦¬ ìˆ«ì ì¶”ì¶œ (ì˜ˆ: "500m", "1km")
                        for cell in row:
                            match = re.search(r"(\d+)\s*(m|km|ë¯¸í„°|í‚¬ë¡œ)", str(cell))
                            if match:
                                distance = int(match.group(1))
                                unit = match.group(2)
                                if 'km' in unit or 'í‚¬ë¡œ' in unit:
                                    distance *= 1000
                                regs[keyword] = distance
        
        return regs
    
    def _extract_scoring_regulations(
        self,
        section: SectionInfo,
        tables: List[TableExtractionResult]
    ) -> Dict[str, Any]:
        """ë°°ì ê¸°ì¤€ ê·œì • ì¶”ì¶œ"""
        scoring = {}
        
        for table in section.tables:
            if len(table.table_data) < 2:
                continue
            
            # í—¤ë” í–‰ íƒì§€
            header = table.header_row or table.table_data[0]
            
            # "í•­ëª©", "ì ìˆ˜", "ë°°ì " ì—´ ì°¾ê¸°
            item_col = next((i for i, h in enumerate(header) if 'í•­ëª©' in str(h)), None)
            score_col = next((i for i, h in enumerate(header) if 'ì ìˆ˜' in str(h) or 'ë°°ì ' in str(h)), None)
            
            if item_col is not None and score_col is not None:
                for row in table.table_data[1:]:
                    if len(row) > max(item_col, score_col):
                        item = str(row[item_col]).strip()
                        score_text = str(row[score_col]).strip()
                        
                        # ì ìˆ˜ ìˆ«ì ì¶”ì¶œ
                        score_match = re.search(r"(\d+)", score_text)
                        if score_match:
                            scoring[item] = int(score_match.group(1))
        
        return scoring
    
    def _extract_rental_regulations(
        self,
        section: SectionInfo,
        tables: List[TableExtractionResult]
    ) -> Dict[str, Any]:
        """ì„ëŒ€ì¡°ê±´ ê·œì • ì¶”ì¶œ"""
        rental = {
            "ì„ëŒ€ë£Œ": None,
            "ë³´ì¦ê¸ˆ": None,
            "ê³„ì•½ê¸°ê°„": None
        }
        
        # í…ìŠ¤íŠ¸ì—ì„œ ê¸ˆì•¡/ê¸°ê°„ ì¶”ì¶œ
        content = section.content
        
        # ì„ëŒ€ë£Œ
        rent_match = re.search(r"ì„ëŒ€ë£Œ[:\s]+(\d+[\d,]*)\s*ì›", content)
        if rent_match:
            rental["ì„ëŒ€ë£Œ"] = rent_match.group(1).replace(',', '')
        
        # ë³´ì¦ê¸ˆ
        deposit_match = re.search(r"ë³´ì¦ê¸ˆ[:\s]+(\d+[\d,]*)\s*ì›", content)
        if deposit_match:
            rental["ë³´ì¦ê¸ˆ"] = deposit_match.group(1).replace(',', '')
        
        # ê³„ì•½ê¸°ê°„
        period_match = re.search(r"ê³„ì•½ê¸°ê°„[:\s]+(\d+)\s*ë…„", content)
        if period_match:
            rental["ê³„ì•½ê¸°ê°„"] = f"{period_match.group(1)}ë…„"
        
        return rental
    
    def _validate_regulations(self, regulations: Dict[str, Any]) -> Dict[str, Any]:
        """ê·œì • ê²€ì¦"""
        validation = {
            "validation_score": 0,
            "total_checks": 0,
            "passed_checks": 0,
            "missing_items": [],
            "issues": []
        }
        
        # ì²´í¬ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ì¦
        for section_name, required_items in self.VALIDATION_CHECKLIST.items():
            if section_name in regulations:
                section_data = regulations[section_name]
                
                for item in required_items:
                    validation["total_checks"] += 1
                    
                    if item in section_data and section_data[item] is not None:
                        validation["passed_checks"] += 1
                    else:
                        validation["missing_items"].append(f"{section_name}.{item}")
        
        # ì ìˆ˜ ê³„ì‚°
        if validation["total_checks"] > 0:
            validation["validation_score"] = round(
                validation["passed_checks"] / validation["total_checks"] * 100,
                1
            )
        
        return validation
    
    async def _save_to_json(self, document: LHNoticeDocument) -> None:
        """JSON ì €ì¥"""
        json_path = self.json_dir / f"{document.document_id}.json"
        
        # Dataclassë¥¼ dictë¡œ ë³€í™˜
        doc_dict = {
            "document_id": document.document_id,
            "filename": document.filename,
            "region": document.region,
            "year": document.year,
            "round": document.round,
            "total_pages": document.total_pages,
            "parsed_at": document.parsed_at,
            "sections": [
                {
                    "section_id": s.section_id,
                    "title": s.title,
                    "page_start": s.page_start,
                    "page_end": s.page_end,
                    "content": s.content[:500],  # ì²˜ìŒ 500ìë§Œ
                    "table_count": len(s.tables),
                    "subsections": s.subsections
                }
                for s in document.sections
            ],
            "regulations": document.regulations,
            "table_summary": {
                "total_tables": len(document.all_tables),
                "by_method": {
                    "pdfplumber": len([t for t in document.all_tables if t.extraction_method == "pdfplumber"]),
                    "tabula": len([t for t in document.all_tables if t.extraction_method == "tabula"]),
                    "pymupdf": len([t for t in document.all_tables if t.extraction_method == "pymupdf"])
                },
                "avg_confidence": round(
                    sum(t.confidence_score for t in document.all_tables) / len(document.all_tables)
                    if document.all_tables else 0,
                    3
                )
            },
            "validation_result": document.validation_result
        }
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(doc_dict, f, ensure_ascii=False, indent=2)
        
        logger.info(f"  ğŸ’¾ JSON ì €ì¥: {json_path}")
    
    def _parse_filename(self, filename: str) -> Dict[str, Any]:
        """íŒŒì¼ëª… íŒŒì‹±"""
        patterns = [
            r"(?P<region>[ê°€-í£]+)(?P<year>\d{2,4})-(?P<round>\d+)ì°¨",
            r"(?P<region>[ê°€-í£]+)_(?P<year>\d{4})_(?P<round>\d+)ì°¨",
            r"(?P<year>\d{4})-(?P<region>[ê°€-í£]+)-(?P<round>\d+)ì°¨"
        ]
        
        for pattern in patterns:
            match = re.search(pattern, filename)
            if match:
                groups = match.groupdict()
                year = int(groups["year"])
                if year < 100:
                    year += 2000
                
                return {
                    "region": groups["region"],
                    "year": year,
                    "round": f"{groups['round']}ì°¨",
                    "version_id": f"{groups['region']}_{year}_{groups['round']}ì°¨"
                }
        
        return {
            "region": "ì „êµ­",
            "year": 2024,
            "round": "1ì°¨",
            "version_id": "unknown"
        }
    
    def _get_page_count(self, pdf_path: str) -> int:
        """í˜ì´ì§€ ìˆ˜ ê°€ì ¸ì˜¤ê¸°"""
        if PDFPLUMBER_AVAILABLE:
            try:
                with pdfplumber.open(pdf_path) as pdf:
                    return len(pdf.pages)
            except:
                pass
        
        if PYMUPDF_AVAILABLE:
            try:
                doc = fitz.open(pdf_path)
                count = len(doc)
                doc.close()
                return count
            except:
                pass
        
        return 0
    
    def _detect_lh_template(self, full_text: str) -> str:
        """
        v2.1: LH í…œí”Œë¦¿ ìë™ ê°ì§€
        
        Returns:
            "2023", "2024", "2025" ë˜ëŠ” "unknown"
        """
        for year, template_info in self.LH_TEMPLATES.items():
            # ì—°ë„ ì‹ë³„ì í™•ì¸
            for identifier in template_info["identifier"]:
                if identifier in full_text:
                    logger.info(f"  ğŸ“‹ LH í…œí”Œë¦¿ ê°ì§€: {year}ë…„")
                    return year
            
            # ì„¹ì…˜ í‚¤ì›Œë“œ ê¸°ë°˜ í™•ì¸ (2ê°œ ì´ìƒ ë§¤ì¹­ ì‹œ)
            matched_keywords = sum(
                1 for keyword in template_info["section_keywords"]
                if keyword in full_text
            )
            
            if matched_keywords >= 2:
                logger.info(f"  ğŸ“‹ LH í…œí”Œë¦¿ ê°ì§€ (í‚¤ì›Œë“œ ê¸°ë°˜): {year}ë…„")
                return year
        
        logger.warning("  âš ï¸ LH í…œí”Œë¦¿ ë¯¸ì‹ë³„, ê¸°ë³¸ê°’(2024) ì‚¬ìš©")
        return "2024"
    
    async def _extract_with_ocr(
        self,
        pdf_path: str,
        skip_pages: set
    ) -> List[TableExtractionResult]:
        """
        v2.1: OCRë¡œ ì´ë¯¸ì§€ ê¸°ë°˜ PDF ì²˜ë¦¬
        
        ì´ë¯¸ì§€ PDFì˜ ê²½ìš°:
        1. PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
        2. Tesseract OCR ì ìš©
        3. í…ìŠ¤íŠ¸ì—ì„œ í‘œ êµ¬ì¡° íƒì§€
        """
        if not TESSERACT_AVAILABLE or not PYMUPDF_AVAILABLE:
            return []
        
        tables = []
        
        try:
            doc = fitz.open(pdf_path)
            
            for page_num in range(len(doc)):
                if (page_num + 1) in skip_pages:
                    continue
                
                # ì´ë¯¸ì§€ ê¸°ë°˜ í˜ì´ì§€ íƒì§€
                page = doc[page_num]
                text = page.get_text()
                
                # í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ì—†ìœ¼ë©´ ì´ë¯¸ì§€ PDFë¡œ íŒë‹¨
                if len(text.strip()) < 50:
                    logger.info(f"    ğŸ–¼ï¸ ì´ë¯¸ì§€ PDF ê°ì§€: í˜ì´ì§€ {page_num + 1}, OCR ì ìš©")
                    
                    # PDF í˜ì´ì§€ â†’ ì´ë¯¸ì§€ ë³€í™˜
                    pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x í™•ëŒ€
                    img_path = self.ocr_dir / f"page_{page_num + 1}.png"
                    pix.save(str(img_path))
                    
                    # OCR ì ìš©
                    ocr_text = pytesseract.image_to_string(
                        Image.open(img_path),
                        lang='kor+eng'  # í•œê¸€ + ì˜ë¬¸
                    )
                    
                    logger.debug(f"      OCR ì¶”ì¶œ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(ocr_text)}ì")
                    
                    # OCR í…ìŠ¤íŠ¸ì—ì„œ í‘œ íƒì§€
                    lines = ocr_text.split('\n')
                    potential_tables = self._detect_table_from_text(lines)
                    
                    for table_idx, table_data in enumerate(potential_tables):
                        confidence = 0.6  # OCRì€ ì¤‘ê°„ ì‹ ë¢°ë„
                        
                        table_result = TableExtractionResult(
                            table_id=f"T{page_num+1:03d}_{table_idx+1:02d}_ocr",
                            page_number=page_num + 1,
                            table_data=table_data,
                            row_count=len(table_data),
                            column_count=len(table_data[0]) if table_data else 0,
                            extraction_method="ocr",
                            confidence_score=confidence,
                            header_row=table_data[0] if table_data else None
                        )
                        
                        tables.append(table_result)
            
            doc.close()
        
        except Exception as e:
            logger.error(f"âŒ OCR ì˜¤ë¥˜: {e}")
        
        return tables
    
    def _extract_exclusion_criteria(
        self,
        sections: List[SectionInfo],
        tables: List[TableExtractionResult]
    ) -> Dict[str, Any]:
        """
        v2.1: ì œì™¸ ê¸°ì¤€ ìë™ ì¶”ì¶œ (95%+ ì •í™•ë„ ëª©í‘œ)
        
        ì¶”ì¶œ í•­ëª©:
        1. ìš©ë„ì§€ì—­ ì œì™¸ (ê³µì—…ì§€ì—­, ë…¹ì§€ì§€ì—­ ë“±)
        2. ê·œì œ ì œì™¸ (ë°©í™”ì§€êµ¬, ë¬¸í™”ì¬ë³´í˜¸êµ¬ì—­ ë“±)
        3. ê±°ë¦¬ ì œì™¸ (ì—­ì„¸ê¶Œ 2km ì´ˆê³¼ ë“±)
        4. ë©´ì  ì œì™¸ (ìµœì†Œ/ìµœëŒ€ ë©´ì )
        """
        exclusion = {
            "zone_exclusions": [],
            "regulation_exclusions": [],
            "distance_exclusions": [],
            "area_exclusions": [],
            "other_exclusions": []
        }
        
        # ì œì™¸ ê¸°ì¤€ ì„¹ì…˜ ì°¾ê¸°
        exclusion_section = next(
            (s for s in sections if "ì œì™¸" in s.title or "íƒˆë½" in s.title),
            None
        )
        
        if not exclusion_section:
            logger.warning("  âš ï¸ ì œì™¸ ê¸°ì¤€ ì„¹ì…˜ ë¯¸ë°œê²¬")
            return exclusion
        
        content = exclusion_section.content
        
        # 1. ìš©ë„ì§€ì—­ ì œì™¸
        zone_patterns = [
            r"([^\s]+ì§€ì—­).*?(?:ì œì™¸|ë¶ˆê°€|íƒˆë½)",
            r"(?:ì œì™¸|ë¶ˆê°€|íƒˆë½).*?([^\s]+ì§€ì—­)"
        ]
        for pattern in zone_patterns:
            matches = re.findall(pattern, content)
            exclusion["zone_exclusions"].extend(matches)
        
        # 2. ê·œì œ ì œì™¸
        regulation_patterns = [
            r"(ë°©í™”ì§€êµ¬|ê³ ë„ì§€êµ¬|ë¬¸í™”ì¬ë³´í˜¸êµ¬ì—­|ì¬ê°œë°œêµ¬ì—­|ì¬ê±´ì¶•êµ¬ì—­)",
            r"(êµ°ì‚¬ì‹œì„¤ë³´í˜¸êµ¬ì—­|ìˆ˜ìš©ë¶€ì§€|ë„ì‹œê³„íšì‹œì„¤)"
        ]
        for pattern in regulation_patterns:
            matches = re.findall(pattern, content)
            exclusion["regulation_exclusions"].extend(matches)
        
        # 3. ê±°ë¦¬ ì œì™¸
        distance_patterns = [
            r"(ì§€í•˜ì² |ì—­ì„¸ê¶Œ).*?(\d+(?:\.\d+)?)\s*(km|m|ë¯¸í„°|í‚¬ë¡œ).*?(?:ì´ˆê³¼|ì´ìƒ|ì´ìƒì‹œ|ë„˜ëŠ”)",
            r"(\d+(?:\.\d+)?)\s*(km|m|ë¯¸í„°|í‚¬ë¡œ).*?(ì§€í•˜ì² |ì—­ì„¸ê¶Œ).*?(?:ì´ˆê³¼|ì´ìƒ)"
        ]
        for pattern in distance_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if len(match) >= 3:
                    distance = float(match[1] if match[1].replace('.', '').isdigit() else match[0])
                    unit = match[2] if len(match) > 2 else match[1]
                    facility = match[0] if not match[0].replace('.', '').isdigit() else match[2]
                    
                    if 'km' in unit or 'í‚¬ë¡œ' in unit:
                        distance *= 1000
                    
                    exclusion["distance_exclusions"].append({
                        "facility": facility,
                        "max_distance_m": distance,
                        "text": f"{facility} {distance}m ì´ˆê³¼ ì œì™¸"
                    })
        
        # 4. ë©´ì  ì œì™¸
        area_patterns = [
            r"(\d+[\d,]*)\s*(?:í‰|ã¡|ì œê³±ë¯¸í„°).*?(?:ë¯¸ë§Œ|ì´í•˜|ì‘ì€)",
            r"(\d+[\d,]*)\s*(?:í‰|ã¡|ì œê³±ë¯¸í„°).*?(?:ì´ˆê³¼|ì´ìƒ|í°)"
        ]
        for pattern in area_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                area = match.replace(',', '') if isinstance(match, str) else match
                exclusion["area_exclusions"].append(area)
        
        # ì¤‘ë³µ ì œê±°
        exclusion["zone_exclusions"] = list(set(exclusion["zone_exclusions"]))
        exclusion["regulation_exclusions"] = list(set(exclusion["regulation_exclusions"]))
        
        logger.info(
            f"  âœ… ì œì™¸ ê¸°ì¤€ ì¶”ì¶œ: "
            f"ìš©ë„ {len(exclusion['zone_exclusions'])}ê°œ, "
            f"ê·œì œ {len(exclusion['regulation_exclusions'])}ê°œ, "
            f"ê±°ë¦¬ {len(exclusion['distance_exclusions'])}ê°œ"
        )
        
        return exclusion
    
    def _extract_agreement_terms(
        self,
        sections: List[SectionInfo],
        tables: List[TableExtractionResult]
    ) -> Dict[str, Any]:
        """
        v2.1: í˜‘ì•½ ì¡°ê±´ ìë™ ì •ê·œí™”
        
        ì¶”ì¶œ í•­ëª©:
        1. ê±´ì¶• ì°©ê³µ ê¸°í•œ
        2. ì„ëŒ€ ê°œì‹œ ê¸°í•œ
        3. ìœ„ì•½ê¸ˆ ì¡°ê±´
        4. ë§¤ì… ì¡°ê±´
        """
        agreement = {
            "construction_deadline": None,
            "rental_start_deadline": None,
            "penalty_conditions": [],
            "purchase_conditions": []
        }
        
        # í˜‘ì•½ ì¡°ê±´ ì„¹ì…˜ ì°¾ê¸°
        agreement_section = next(
            (s for s in sections if "í˜‘ì•½" in s.title or "ê³„ì•½" in s.title),
            None
        )
        
        if not agreement_section:
            return agreement
        
        content = agreement_section.content
        
        # 1. ì°©ê³µ ê¸°í•œ
        construction_match = re.search(
            r"ì°©ê³µ.*?(\d+)\s*(ê°œì›”|ë…„|ì¼)",
            content
        )
        if construction_match:
            agreement["construction_deadline"] = f"{construction_match.group(1)}{construction_match.group(2)}"
        
        # 2. ì„ëŒ€ ê°œì‹œ ê¸°í•œ
        rental_match = re.search(
            r"ì„ëŒ€.*?ê°œì‹œ.*?(\d+)\s*(ê°œì›”|ë…„|ì¼)",
            content
        )
        if rental_match:
            agreement["rental_start_deadline"] = f"{rental_match.group(1)}{rental_match.group(2)}"
        
        # 3. ìœ„ì•½ê¸ˆ
        penalty_matches = re.findall(
            r"ìœ„ì•½ê¸ˆ.*?(\d+[\d,]*)\s*(?:ì›|ë§Œì›|ì–µì›)",
            content
        )
        agreement["penalty_conditions"] = [p.replace(',', '') for p in penalty_matches]
        
        logger.info(f"  âœ… í˜‘ì•½ ì¡°ê±´ ì¶”ì¶œ: {len(agreement)}ê°œ í•­ëª©")
        
        return agreement


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_lh_notice_loader_v21 = None


def get_lh_notice_loader_v21() -> LHNoticeLoaderV21:
    """LH Notice Loader v2.1 ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _lh_notice_loader_v21
    if _lh_notice_loader_v21 is None:
        _lh_notice_loader_v21 = LHNoticeLoaderV21()
    return _lh_notice_loader_v21
