"""
ZeroSite v18 Phase 4 - Real Transaction Cache System
=====================================================
ì‹¤ê±°ë˜ê°€ API í˜¸ì¶œ ìµœì í™” - íŒŒì¼ ê¸°ë°˜ ìºì‹±

Features:
- JSON íŒŒì¼ ê¸°ë°˜ ìºì‹± (SQLite ëŒ€ì•ˆ)
- TTL ê¸°ë°˜ ìë™ ë§Œë£Œ (ê¸°ë³¸ 24ì‹œê°„)
- ì§€ì—­/ê¸°ê°„ë³„ ìºì‹œ í‚¤ ê´€ë¦¬
- í†µê³„ ë° ëª¨ë‹ˆí„°ë§
"""

import json
import hashlib
from pathlib import Path
from typing import Optional, Dict, List, Any
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
import logging

logger = logging.getLogger(__name__)


@dataclass
class CacheMetadata:
    """ìºì‹œ ë©”íƒ€ë°ì´í„°"""
    key: str
    created_at: str
    expires_at: str
    hit_count: int = 0
    data_size: int = 0


class RealTransactionCache:
    """
    ì‹¤ê±°ë˜ê°€ API ìºì‹± ì‹œìŠ¤í…œ
    
    Cache Structure:
    - cache/real_transaction/
        - land_{region_code}_{year_month}.json
        - building_{region_code}_{year_month}.json
        - metadata.json
    """
    
    def __init__(self, cache_dir: str = "cache/real_transaction", ttl_hours: int = 24):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.ttl_hours = ttl_hours
        self.metadata_file = self.cache_dir / "metadata.json"
        self._load_metadata()
        
        logger.info("=" * 80)
        logger.info("ğŸ’¾ RealTransactionCache initialized")
        logger.info(f"   Cache directory: {self.cache_dir.absolute()}")
        logger.info(f"   TTL: {ttl_hours} hours")
        logger.info(f"   Cached entries: {len(self.metadata)}")
        logger.info("=" * 80)
    
    def _load_metadata(self):
        """ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        if self.metadata_file.exists():
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.metadata = {k: CacheMetadata(**v) for k, v in data.items()}
        else:
            self.metadata = {}
    
    def _save_metadata(self):
        """ë©”íƒ€ë°ì´í„° ì €ì¥"""
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump({k: asdict(v) for k, v in self.metadata.items()}, 
                     f, indent=2, ensure_ascii=False)
    
    def _generate_key(self, cache_type: str, region_code: str, year_month: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        raw_key = f"{cache_type}_{region_code}_{year_month}"
        return hashlib.md5(raw_key.encode()).hexdigest()
    
    def _get_cache_file_path(self, key: str) -> Path:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ"""
        return self.cache_dir / f"{key}.json"
    
    def get(self, cache_type: str, region_code: str, year_month: str) -> Optional[List[Dict]]:
        """
        ìºì‹œ ì¡°íšŒ
        
        Args:
            cache_type: 'land' or 'building'
            region_code: ì§€ì—­ì½”ë“œ (ì˜ˆ: '11110')
            year_month: ì¡°íšŒ ë…„ì›” (ì˜ˆ: '202412')
        
        Returns:
            ìºì‹œëœ ë°ì´í„° ë˜ëŠ” None
        """
        key = self._generate_key(cache_type, region_code, year_month)
        
        # ë©”íƒ€ë°ì´í„° í™•ì¸
        if key not in self.metadata:
            logger.debug(f"âŒ Cache MISS: {cache_type}/{region_code}/{year_month}")
            return None
        
        meta = self.metadata[key]
        
        # TTL í™•ì¸
        expires_at = datetime.fromisoformat(meta.expires_at)
        if datetime.now() > expires_at:
            logger.info(f"â° Cache EXPIRED: {cache_type}/{region_code}/{year_month}")
            self._delete_cache(key)
            return None
        
        # ìºì‹œ íŒŒì¼ ì½ê¸°
        cache_file = self._get_cache_file_path(key)
        if not cache_file.exists():
            logger.warning(f"âš ï¸ Cache file missing: {key}")
            del self.metadata[key]
            self._save_metadata()
            return None
        
        with open(cache_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Hit count ì—…ë°ì´íŠ¸
        meta.hit_count += 1
        self._save_metadata()
        
        logger.info(f"âœ… Cache HIT: {cache_type}/{region_code}/{year_month} (hit #{meta.hit_count})")
        return data
    
    def set(self, cache_type: str, region_code: str, year_month: str, data: List[Dict]):
        """
        ìºì‹œ ì €ì¥
        
        Args:
            cache_type: 'land' or 'building'
            region_code: ì§€ì—­ì½”ë“œ
            year_month: ì¡°íšŒ ë…„ì›”
            data: ì €ì¥í•  ë°ì´í„°
        """
        key = self._generate_key(cache_type, region_code, year_month)
        
        # ë°ì´í„° ì €ì¥
        cache_file = self._get_cache_file_path(key)
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        now = datetime.now()
        expires_at = now + timedelta(hours=self.ttl_hours)
        
        self.metadata[key] = CacheMetadata(
            key=key,
            created_at=now.isoformat(),
            expires_at=expires_at.isoformat(),
            hit_count=0,
            data_size=len(data)
        )
        self._save_metadata()
        
        logger.info(f"ğŸ’¾ Cache SAVED: {cache_type}/{region_code}/{year_month} ({len(data)} records)")
    
    def _delete_cache(self, key: str):
        """ìºì‹œ ì‚­ì œ"""
        cache_file = self._get_cache_file_path(key)
        if cache_file.exists():
            cache_file.unlink()
        
        if key in self.metadata:
            del self.metadata[key]
            self._save_metadata()
    
    def clear_expired(self):
        """ë§Œë£Œëœ ìºì‹œ ì •ë¦¬"""
        now = datetime.now()
        expired_keys = []
        
        for key, meta in self.metadata.items():
            expires_at = datetime.fromisoformat(meta.expires_at)
            if now > expires_at:
                expired_keys.append(key)
        
        for key in expired_keys:
            self._delete_cache(key)
        
        if expired_keys:
            logger.info(f"ğŸ—‘ï¸ Cleared {len(expired_keys)} expired cache entries")
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„"""
        total_entries = len(self.metadata)
        total_hits = sum(m.hit_count for m in self.metadata.values())
        total_size = sum(m.data_size for m in self.metadata.values())
        
        # ë§Œë£Œ ì‹œê°„ë³„ ë¶„ë¥˜
        now = datetime.now()
        valid_entries = 0
        expired_entries = 0
        
        for meta in self.metadata.values():
            expires_at = datetime.fromisoformat(meta.expires_at)
            if now <= expires_at:
                valid_entries += 1
            else:
                expired_entries += 1
        
        return {
            "total_entries": total_entries,
            "valid_entries": valid_entries,
            "expired_entries": expired_entries,
            "total_hits": total_hits,
            "total_records": total_size,
            "cache_dir": str(self.cache_dir.absolute()),
            "ttl_hours": self.ttl_hours
        }
    
    def print_stats(self):
        """ìºì‹œ í†µê³„ ì¶œë ¥"""
        stats = self.get_stats()
        
        print("\n" + "=" * 80)
        print("ğŸ“Š Real Transaction Cache Statistics")
        print("=" * 80)
        print(f"Total Entries:    {stats['total_entries']}")
        print(f"Valid Entries:    {stats['valid_entries']}")
        print(f"Expired Entries:  {stats['expired_entries']}")
        print(f"Total Hits:       {stats['total_hits']}")
        print(f"Total Records:    {stats['total_records']}")
        print(f"Cache Directory:  {stats['cache_dir']}")
        print(f"TTL:              {stats['ttl_hours']} hours")
        print("=" * 80 + "\n")


# Singleton instance
_cache_instance = None

def get_cache(ttl_hours: int = 24) -> RealTransactionCache:
    """ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    global _cache_instance
    if _cache_instance is None:
        _cache_instance = RealTransactionCache(ttl_hours=ttl_hours)
    return _cache_instance
